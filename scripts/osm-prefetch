#!/usr/bin/env bash
# Prefetch Geofabrik region files into a local mirror directory.
#
# Downloads all unique region files referenced in REGION_REGISTRY to avoid
# hammering download.geofabrik.de during test runs.  The mirror can then be
# used at runtime by setting AFL_GEOFABRIK_MIRROR to the mirror directory.
#
# Usage:
#   scripts/osm-prefetch                                      # download all (~250 files)
#   scripts/osm-prefetch --dry-run                            # list files without downloading
#   scripts/osm-prefetch --mirror-dir /data/osm-mirror        # custom directory
#   scripts/osm-prefetch --include "europe/monaco|antarctica" # subset only
#   scripts/osm-prefetch --resume                             # skip existing files
#   scripts/osm-prefetch --fmt shp                            # shapefiles instead of pbf
#   scripts/osm-prefetch --help                               # show all options

set -euo pipefail

REPO_ROOT="$(cd "$(dirname "$0")/.." && pwd)"

PYTHON="${REPO_ROOT}/.venv/bin/python3"
[[ -x "$PYTHON" ]] || PYTHON=python3

exec env PYTHONPATH="${REPO_ROOT}:${REPO_ROOT}/examples/osm-geocoder" "$PYTHON" - "$@" <<'PYEOF'
"""Inline Python script for osm-prefetch."""

import argparse
import json
import os
import re
import sys
import time
import urllib.request
from datetime import UTC, datetime

from handlers.cache_handlers import REGION_REGISTRY

GEOFABRIK_BASE = "https://download.geofabrik.de"
FORMAT_EXTENSIONS = {
    "pbf": "osm.pbf",
    "shp": "free.shp.zip",
}
USER_AGENT = "AgentFlow-OSM-Prefetch/1.0"


def extract_unique_paths() -> list[str]:
    """Extract and deduplicate all Geofabrik paths from REGION_REGISTRY."""
    seen: set[str] = set()
    for facet_map in REGION_REGISTRY.values():
        for path in facet_map.values():
            seen.add(path)
    return sorted(seen)


def filter_paths(
    paths: list[str],
    include: str | None,
    exclude: str | None,
) -> list[str]:
    """Apply --include/--exclude regex filters."""
    if include:
        pat = re.compile(include)
        paths = [p for p in paths if pat.search(p)]
    if exclude:
        pat = re.compile(exclude)
        paths = [p for p in paths if not pat.search(p)]
    return paths


def download_file(url: str, dest: str) -> int:
    """Download a URL to a local file.  Returns file size in bytes."""
    os.makedirs(os.path.dirname(dest), exist_ok=True)
    tmp = dest + f".tmp.{os.getpid()}"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
        with urllib.request.urlopen(req, timeout=300) as resp, open(tmp, "wb") as f:
            while True:
                chunk = resp.read(8192)
                if not chunk:
                    break
                f.write(chunk)
        os.replace(tmp, dest)
    except BaseException:
        try:
            os.remove(tmp)
        except OSError:
            pass
        raise
    return os.path.getsize(dest)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Prefetch Geofabrik region files into a local mirror.",
    )
    parser.add_argument(
        "--mirror-dir",
        default="./osm-mirror",
        help="Target directory for downloaded files (default: ./osm-mirror)",
    )
    parser.add_argument(
        "--fmt",
        choices=["pbf", "shp", "all"],
        default="pbf",
        help="Format: pbf, shp, or all (default: pbf)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="List files without downloading",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=2.0,
        help="Delay in seconds between downloads (default: 2)",
    )
    parser.add_argument(
        "--include",
        help="Only download paths matching this regex",
    )
    parser.add_argument(
        "--exclude",
        help="Skip paths matching this regex",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Skip files that already exist in the mirror",
    )
    args = parser.parse_args()

    mirror_dir = os.path.abspath(args.mirror_dir)
    fmts = list(FORMAT_EXTENSIONS.keys()) if args.fmt == "all" else [args.fmt]

    paths = extract_unique_paths()
    paths = filter_paths(paths, args.include, args.exclude)

    # Build work list: (url, relative_path, absolute_dest)
    work: list[tuple[str, str, str]] = []
    for path in paths:
        for fmt in fmts:
            ext = FORMAT_EXTENSIONS[fmt]
            rel = f"{path}-latest.{ext}"
            url = f"{GEOFABRIK_BASE}/{rel}"
            dest = os.path.join(mirror_dir, rel)
            work.append((url, rel, dest))

    print(f"Mirror directory: {mirror_dir}")
    print(f"Unique regions:   {len(paths)}")
    print(f"Files to process: {len(work)}")
    print()

    if args.dry_run:
        for url, rel, _ in work:
            print(f"  {rel}  ({url})")
        print(f"\nDry run â€” {len(work)} file(s) would be downloaded.")
        return

    downloaded = 0
    skipped = 0
    errors = 0
    total_bytes = 0
    manifest_files: dict[str, str] = {}

    for i, (url, rel, dest) in enumerate(work):
        label = f"[{i + 1}/{len(work)}]"

        if args.resume and os.path.isfile(dest):
            size = os.path.getsize(dest)
            total_bytes += size
            skipped += 1
            manifest_files[url] = rel
            print(f"  {label} SKIP (exists) {rel}")
            continue

        print(f"  {label} Downloading {rel} ...", end="", flush=True)
        try:
            size = download_file(url, dest)
            downloaded += 1
            total_bytes += size
            manifest_files[url] = rel
            print(f" {size:,} bytes")
        except Exception as exc:
            errors += 1
            print(f" ERROR: {exc}")

        if i < len(work) - 1 and args.delay > 0:
            time.sleep(args.delay)

    # Write manifest
    manifest = {
        "generated": datetime.now(UTC).isoformat(),
        "mirror_dir": mirror_dir,
        "format": args.fmt,
        "file_count": len(manifest_files),
        "files": manifest_files,
    }
    manifest_path = os.path.join(mirror_dir, "manifest.json")
    os.makedirs(mirror_dir, exist_ok=True)
    with open(manifest_path, "w") as f:
        json.dump(manifest, f, indent=2)
        f.write("\n")

    # Summary
    def human_size(n: int) -> str:
        for unit in ("B", "KB", "MB", "GB"):
            if n < 1024:
                return f"{n:.1f} {unit}"
            n /= 1024
        return f"{n:.1f} TB"

    print()
    print(f"Downloaded: {downloaded}")
    print(f"Skipped:    {skipped}")
    print(f"Errors:     {errors}")
    print(f"Total size: {human_size(total_bytes)}")
    print(f"Manifest:   {manifest_path}")

    if errors > 0:
        sys.exit(1)


main()
PYEOF
