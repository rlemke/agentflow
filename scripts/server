#!/usr/bin/env bash
# Start the AFL agent runner server.
#
# The server executes workflows by dispatching event facets to Claude
# or custom-registered tool handlers via the ClaudeAgentRunner.
#
# Usage:
#   scripts/server                                  # start with defaults
#   scripts/server --config /path/to/afl.config.json
#   scripts/server --workflow MyWorkflow --input '{"x": 1}'
#
# Options:
#   --config FILE        Path to AFL config file
#   --workflow NAME      Workflow name to execute
#   --input JSON         JSON input parameters
#   --flow-id ID         Flow ID to load from MongoDB
#   --help               Show this help message

set -euo pipefail

REPO_ROOT="$(cd "$(dirname "$0")/.." && pwd)"

PYTHON="${REPO_ROOT}/.venv/bin/python3"
[[ -x "$PYTHON" ]] || PYTHON=python3

CONFIG_ARG=""
WORKFLOW=""
INPUT="{}"
FLOW_ID=""

while [[ $# -gt 0 ]]; do
    case "$1" in
        --config)   CONFIG_ARG="$2"; shift 2 ;;
        --workflow) WORKFLOW="$2"; shift 2 ;;
        --input)    INPUT="$2"; shift 2 ;;
        --flow-id)  FLOW_ID="$2"; shift 2 ;;
        --help|-h)
            awk 'NR==1{next} /^#/{sub(/^# ?/,""); print; next} {exit}' "$0"
            exit 0
            ;;
        *) echo "Unknown option: $1" >&2; exit 1 ;;
    esac
done

if [[ -z "$WORKFLOW" && -z "$FLOW_ID" ]]; then
    echo "Error: --workflow NAME or --flow-id ID is required." >&2
    echo "Run 'scripts/server --help' for usage." >&2
    exit 1
fi

exec env PYTHONPATH="$REPO_ROOT" "$PYTHON" -c "
import json
import sys

from afl.config import load_config
from afl.runtime.mongo_store import MongoStore
from afl.runtime import Evaluator, Telemetry

config_path = '${CONFIG_ARG}' or None
config = load_config(config_path)
store = MongoStore.from_config(config.mongodb)
telemetry = Telemetry(enabled=True)
evaluator = Evaluator(persistence=store, telemetry=telemetry)

workflow_name = '${WORKFLOW}'
flow_id = '${FLOW_ID}'
inputs = json.loads('${INPUT}')

# Resolve workflow
if flow_id:
    flow = store.get_flow(flow_id)
    if not flow:
        print(f'Error: flow {flow_id} not found', file=sys.stderr)
        sys.exit(1)
    workflows = store.get_workflows_by_flow(flow_id)
    if workflow_name:
        wf = next((w for w in workflows if w.name == workflow_name), None)
    else:
        wf = workflows[0] if workflows else None
elif workflow_name:
    wf = store.get_workflow_by_name(workflow_name)
else:
    wf = None

if not wf:
    print(f'Error: workflow not found', file=sys.stderr)
    sys.exit(1)

print(f'Starting workflow: {wf.name} (id={wf.uuid})')
print(f'Inputs: {json.dumps(inputs)}')

# Load the compiled flow for the AST
flow = store.get_flow(wf.flow_id)
if not flow:
    print(f'Error: flow {wf.flow_id} not found for workflow', file=sys.stderr)
    sys.exit(1)

# Parse the AFL source to get the workflow AST
from afl.parser import AFLParser
from afl.emitter import JSONEmitter

if not flow.compiled_sources:
    print('Error: no compiled sources in flow', file=sys.stderr)
    sys.exit(1)

parser = AFLParser()
ast = parser.parse(flow.compiled_sources[0].content)
emitter = JSONEmitter(include_locations=False)
program_dict = json.loads(emitter.emit(ast))

# Find matching workflow in compiled output
wf_ast = None
for w in program_dict.get('workflows', []):
    if w['name'] == wf.name:
        wf_ast = w
        break

if not wf_ast:
    print(f'Error: workflow {wf.name} not found in compiled AST', file=sys.stderr)
    sys.exit(1)

result = evaluator.execute(wf_ast, inputs=inputs)

if result.success:
    print(f'Completed successfully.')
    print(f'Outputs: {json.dumps(result.outputs, default=str)}')
else:
    print(f'Failed: {result.error}', file=sys.stderr)
    sys.exit(1)
"
